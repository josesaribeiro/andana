---
layout: post
title: "The unreasonable usefulness of entropy"
author: "tomrod"
use_math: true
tags: [math, probability, information, entropy, ML fundamentals, ]
---
# DRAFT

## Context

Entropy is the average rate that information is revealed by a data-generated process. Mathematically, it is defined[^1] for a probability mass function $P_i$ as

$$S = - \sum_i P(x_i) \log(P(x_i))$$

For such a simple item, it is *incredibly and unreasonably useful*, for the simple reason that it lets us find the signal in the noise. As such, it is essential in crafting narratives.

This post will review the interpretation and some uses of entropy measures.

## Interpretation
Entropy basically says that the more information you know about something, the less new information one gets from observation. As an example, consider coin flipping. If you have a fair coin ( $$P(H)$$ = 50%) then there is no way to predict the outcome of the next coin flip. In this case, the number entropy bits are:
$$S_{even} = -\log_2(P(H)) \cdot P(H) - \log_2(1-P(H))\cdot(1-P(H)) = 1$$
This is the *maximum entropy* a coin flip can have. Alternatively, if you have a coin that has both tails, meaning the probability of heads $P(H)=0$, and you get
$S_{degen} =0$.[^2] If your coin is unfair but not all heads or all tails, your entropy falls below the maximum but is not minimized. 
 You can see this using the following

```python
x = np.linspace(.0001, .9999, 400)
y = -np.log(x) * x + -np.log(1-x)*(1-x)
plt.plot(x,y)
plt.show()
```
![png](/assets/images/20181230_basic_entropy.png)



## Uses
As seen, the formal definition of entropy is pretty simple. However, the concept is far broader. Entropy is not just used for looking at a single probability distribution. You can also use it to see how much information one distribution gives you about another. You can use this in determining temporal relationships, as an ML objective function, in cryptography, in linguistics (and NLP type problems generally), and so forth. Basically, it is a framework where learning and communication occur.

### Cross Entropy
Continuing the theme of 


### Kullback-Liebler divergence
One of the broadest use cases for entropy is the *Kullback-Liebler* (KL) divergence statistic, most commonly seen as $$D_{KL}(P||Q)$$. Formally, the KL divergence is the improvement of a cross-entropy over a normalizing entropy:

$$ D_{KL}(P||Q) = S_{qp} - S_{p} = \sum_i p(x_i)\cdot(-\log(q(x_i))- (-\log(p(x_i)))) = \sum_i p(x_i)\cdot\log(\frac{p(x_i)}{q(x_i)}).$$[\3]

Basically, it is a measure of surprise (divergence) from $P$ with respect to $Q$. For Bayesian work, $Q$ is taken as the prior and $P$ the posterior.

The asymmetry of KL-divergence is important to remember. Notably, switching the normalizing distribution is not equivalent, $$D_{KL}(P||Q) \neq D_{KL}(Q||P)$$. 

As an example of KL-divergence in action, consider the well-known example that a $$\Xi^2_2$$ distribution is equivalent to an exponential distribution with scale parameter of 2. You can hope this is true by staring at histograms until you go cross-eyed, or working through the probability density function to verify. However, you can use the KL-divergence to verify. The following script shows the verification as well as gives a first-pass at KL-divergence in code.

```python
import numpy as np
from scipy.stats import entropy
from scipy.stats import chi2
from scipy.stats import expon

dist1 = chi2.rvs(2, size=10000)

def ent(p,q):
    mm = min(min(p),min(q))
    MM = max(max(p), max(q))
    d1 = np.histogram(p, bins=100, range=(mm,MM))[0]/len(p)
    d2 = np.histogram(q, bins=100, range=(mm,MM))[0]/len(q)
    agg = 0
    for prob1, prob2 in zip(d1, d2):
        if prob1 ==0 or prob2 == 0:
            continue
        else:
            agg += prob1 * np.log(prob1/prob2)
    return agg


x = np.linspace(0.1,4.1,100)
y = []
for val in x:
    new_dist = expon.rvs(scale=val, size=10000)
    y.append(ent(dist1,new_dist))
    
plt.figure()
plt.title('Entropy across several Exponential Scale Options')
plt.plot(x,y)
plt.show()
```

![png](/assets/images/20181230-entropy_2_1.png)

What are some use cases?
* Measuring loss of information (via approximation)
* Areas of neural networks
* 



### Mutual Information

### Transfer entropy

### Free Energy Minimization

[^1]: $\log$ here is defined loosely -- typically in information theory $\log_2$ is used, though $\ln$ or $\log_{10}$ or any other bases are acceptable.

[^2]: $\log(0)$ is more or less being ignored here. If it makes you more comfortable, remember the term is mulitiplied by zero to begin with. It is derived from the fact that $$\lim_{p\to 0+} p \log(p) = 0$$

[\3] Where the support of $Q$ is a subset of the support of $P$.