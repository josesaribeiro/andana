---
layout: post
title: "The unreasonable usefulness of entropy"
author: "tomrod"
use_math: true
tags: [math, probability, information, entropy, ML fundamentals ]
---

## Context

Entropy is the average rate that information is revealed by a data-generated process. Mathematically, it is defined[^1] for a probability mass function $p(x_i)$ as

$$H(p) = - \sum_i p(x_i) \log(p(x_i))$$

For such a simple item, it is *incredibly and unreasonably useful*, for the simple reason that it lets us find the signal in the noise. As such, it is essential in crafting narratives.

This post will review the interpretation and some uses of entropy measures. There are many more which will come in later posts.

## Interpretation
Entropy basically says that the more information you know about something, the less new information one gets from observation. As an example, consider coin flipping. If you have a fair coin ( $$P(H)$$ = 50%) then there is no way to predict the outcome of the next coin flip. In this case, the number entropy bits are:
$$S_{even} = -\log_2(P(H)) \cdot P(H) - \log_2(1-P(H))\cdot(1-P(H)) = 1$$
This is the *maximum entropy* a coin flip can have. Alternatively, if you have a coin that has both tails, meaning the probability of heads $P(H)=0$, and you get
$S_{degen} =0$.[^2] If your coin is unfair but not all heads or all tails, your entropy falls below the maximum but is not minimized. 
 You can see this using the following

```python
x = np.linspace(.0001, .9999, 400)
y = -np.log(x) * x + -np.log(1-x)*(1-x)
plt.plot(x,y)
plt.show()
```
![png](/assets/images/20181230_basic_entropy.png)



## Uses
As seen, the formal definition of entropy is pretty simple. However, the concept is far broader. Entropy is not just used for looking at a single probability distribution. You can also use it to see how much information one distribution gives you about another. You can use this in determining temporal relationships, as an ML objective function, in cryptography, in linguistics (and NLP type problems generally), and so forth. Basically, it is a framework where learning and communication occur.

### Cross Entropy
Continuing the theme of entropy, we come to a useful notion of *cross-entropy*. Cross entropy is defined formally as

$$H(p,q) = \sum_{i} p(x_i) \log(q(x_i))$$

If entropy is the measure of the expected number of bits "optimially encoding" a distribution, cross-entropy is the expected number of bits encoding a distribution using the *wrong* encoding. In this case, the lower bits the better. There are some obvious correlations with the confusion matrix, which I will discuss in a future post.


### Kullback-Liebler divergence
One of the broadest use cases for entropy is the *Kullback-Liebler* (KL) divergence statistic, most commonly seen as $$D_{KL}(p|q)$$. Formally, the KL divergence is the improvement of a cross-entropy over a normalizing entropy:

$$ D_{KL}(p||q) = S_{qp} - S_{p} = \sum_i p(x_i)\cdot(-\log(q(x_i))- (-\log(p(x_i)))) = \sum_i p(x_i)\cdot\log(\frac{p(x_i)}{q(x_i)}).$$[\3]

Basically, it is a measure of surprise (divergence) from $P$ with respect to $Q$. For Bayesian work, $Q$ is taken as the prior and $P$ the posterior.

The asymmetry of KL-divergence is important to remember. Notably, switching the normalizing distribution is not equivalent, $$D_{KL}(P||Q) \neq D_{KL}(Q||P)$$. 

As an example of KL-divergence in action, consider the well-known example that a $$\Xi^2_2$$ distribution is equivalent to an exponential distribution with scale parameter $$\lambda$$ of 2. You can hope this is true by staring at histograms until you go cross-eyed, or working through the probability density function to verify. However, you can use the KL-divergence to verify. The following script shows the verification as well as gives a first-pass at KL-divergence in code.

```python
import numpy as np
from scipy.stats import entropy
from scipy.stats import chi2
from scipy.stats import expon

dist1 = chi2.rvs(2, size=10000)

def ent(p,q):
    mm = min(min(p),min(q))
    MM = max(max(p), max(q))
    d1 = np.histogram(p, bins=100, range=(mm,MM))[0]/len(p)
    d2 = np.histogram(q, bins=100, range=(mm,MM))[0]/len(q)
    agg = 0
    for prob1, prob2 in zip(d1, d2):
        if prob1 ==0 or prob2 == 0:
            continue
        else:
            agg += prob1 * np.log(prob1/prob2)
    return agg


x = np.linspace(0.1,4.1,100)
y = []
for val in x:
    new_dist = expon.rvs(scale=val, size=10000)
    y.append(ent(dist1,new_dist))
    
plt.figure()
plt.title('Entropy across several Exponential Scale Options')
plt.plot(x,y)
plt.show()
```

![png](/assets/images/20181230-entropy_2_1.png)
Note that the KL does not become precisely 0, but it does minimize with $$\lambda=2$$.

What are some use cases?
* Measuring loss of information (via approximation)
* Comparing a data set to a known distribution -- quite common in data science, economics, and other areas of research
* Neural networks
* [Many more!](https://stats.stackexchange.com/questions/185858/use-of-kl-divergence-in-practice)

## Relationships of Entropy, Cross-entropy, and KL-divergence

You may have already guessed, but cross-entropy is an absolute level of entropy added to the Kullback-Liebler divergence:

$$H(p,q) = H(p) + D_{KL}(p|q)$$

## Footnotes

[^1]: $\log$ here is defined loosely -- typically in information theory $\log_2$ is used, though $\ln$ or $\log_{10}$ or any other bases are acceptable.

[^2]: $\log(0)$ is more or less being ignored here. If it makes you more comfortable, remember the term is mulitiplied by zero to begin with. It is derived from the fact that $$\lim_{p\to 0+} p \log(p) = 0$$

[\3] Where the support of $Q$ is a subset of the support of $P$.