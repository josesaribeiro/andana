---
layout: post
title: "The unreasonable usefulness of entropy"
author: "tomrod"
use_math: true
tags: [math, probability, information, entropy, ML fundamentals, ]
---
# DRAFT

## Context

Entropy is the average rate that information is revealed by a data-generated process. Mathematically, it is defined[^1] for a probability mass function $P_i$ as

$$S = - \sum_i P_i \log(P_i)$$

For such a simple item, it is *incredibly and unreasonably useful*, for the simple reason that it lets us find the signal in the noise. As such, it is essential in crafting narratives.

This post will review the interpretation and some uses of entropy measures.

## Interpretation
Entropy basically says that the more information you know about something, the less new information one gets from observation. As an example, consider coin flipping. If you have a fair coin ( $$P(H)$$ = 50%) then there is no way to predict the outcome of the next coin flip. In this case, the number entropy bits are:
$$S_{even} = -\log_2(P(H)) \cdot P(H) - \log_2(1-P(H))\cdot(1-P(H)) = 1$$
This is the *maximum entropy* a coin flip can have. Alternatively, if you have a coin that has both tails, meaning the probability of heads $P(H)=0$, and you get
$S_{degen} =0$.[^2] If your coin is unfair but not all heads or all tails, your entropy falls below the maximum but is not minimized. 
 You can see this using the following

```python
x = np.linspace(.0001, .9999, 400)
y = -np.log(x) * x + -np.log(1-x)*(1-x)
plt.plot(x,y)
plt.show()
```
![png](/assets/images/20181230_basic_entropy.png)



## Uses
As seen, the formal definition of entropy is pretty simple. However, the concept is far broader. Entropy is not just used for looking at a single probability distribution. You can also use it to see how much information one distribution gives you about another. You can use this in determining temporal relationships, as an ML objective function, and so forth. 


### Kullback-Liebler divergence

### Cross Entropy minimization




### Mutual Information

### Transfer entropy

### Free Energy Minimization

[^1]: $\log$ here is defined loosely -- typically in information theory $\log_2$ is used, though $\ln$ or $\log_{10}$ or any other bases are acceptable.

[^2]: $\log(0)$ is more or less being ignored here. If it makes you more comfortable, remember the term is mulitiplied by zero to begin with. It is derived from the fact that
$\lim_{p\to 0+} p \log(p) = 0$